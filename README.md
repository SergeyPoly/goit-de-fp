# Final project (Data Engineering)

**Завдання 1. Building an End-to-End Streaming Pipeline**

Ви працюєте в букмекерській конторі. Одне з ваших завдань — генерація даних для ML-моделей. Деякі з features — це середній зріст, середня вага, стать, країна походження атлетів.

Моделі тренуються окремо для кожного виду спорту. Дані про наявність медалі у виступі використовуються як target (output) для ML-моделі.

Фірма хоче коригувати коефіцієнти ставок якомога швидше, тому вас попросили розробити відповідне рішення з використанням стримінгу. Фізичні дані атлетів відомі заздалегідь і зберігаються в MySQL базі даних. Результати змагань же подаються через Kafka-топік.

Ваша задача:
1. Зчитати дані фізичних показників атлетів за допомогою Spark з MySQL таблиці olympic_dataset.athlete_bio (база даних і Credentials до неї вам будуть надані).

2. Відфільтрувати дані, де показники зросту та ваги є порожніми або не є числами. Можна це зробити на будь-якому етапі вашої програми.

3. Зчитати дані з mysql таблиці athlete_event_results і записати в кафка топік athlete_event_results. Зчитати дані з результатами змагань з Kafka-топіку athlete_event_results. Дані з json-формату необхідно перевести в dataframe-формат, де кожне поле json є окремою колонкою.

4. Об’єднати дані з результатами змагань з Kafka-топіку з біологічними даними з MySQL таблиці за допомогою ключа athlete_id.

5. Знайти середній зріст і вагу атлетів індивідуально для кожного виду спорту, типу медалі або її відсутності, статі, країни (country_noc). Додайте також timestamp, коли розрахунки були зроблені.

6. Зробіть стрим даних (за допомогою функції forEachBatch) у:
    - вихідний кафка-топік,
    - базу даних.

**Завдання 2. Building an End-to-End Batch Data Lake**

Основне завдання полягає в побудові трирівневої архітектури обробки даних: від початкового збереження (landing zone), через оброблені та очищені дані (bronze/silver), до фінального аналітичного набору (gold).

Ваша задача:
1. Написати файл landing_to_bronze.py. Він має:
    - завантажувати файл з ftp-сервера в оригінальному форматі csv, 
    - за допомогою Spark прочитати csv-файл і зберегти його у форматі parquet у папку bronze/{table}, де {table} — ім’я таблиці.

2. Написати файл bronze_to_silver.py. Він має:
    - зчитувати таблицю bronze,
    - виконувати функцію чистки тексту для всіх текстових колонок,
    - робити дедублікацію рядків,
    - записувати таблицю в папку silver/{table}, де {table} — ім’я таблиці.


3. Написати файл silver_to_gold.py. Він має:
    - зчитувати дві таблиці: silver/athlete_bio та silver/athlete_event_results,
    - робити join за колонкою athlete_id,
    - для кожної комбінації цих 4 стовпчиків (sport, medal, sex, country_noc) знаходити середні значення weight і height,
    - додати колонку timestamp з часовою міткою виконання програми,
    - записувати дані в gold/avg_stats.


4. Написати файл project_solution.py, де буде знаходитись Airflow DAG, який послідовно запускатиме всі три файли.